<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>GPyOpt.optimization.acquisition_optimizer &mdash; GPyOpt  documentation</title>
    
    <link rel="stylesheet" href="../../../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="top" title="GPyOpt  documentation" href="../../../index.html" />
    <link rel="up" title="Module code" href="../../index.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">GPyOpt  documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for GPyOpt.optimization.acquisition_optimizer</h1><div class="highlight"><pre>
<span class="c"># Copyright (c) 2016, the GPyOpt Authors</span>
<span class="c"># Licensed under the BSD 3-clause license (see LICENSE.txt)</span>

<span class="kn">from</span> <span class="nn">.optimizer</span> <span class="kn">import</span> <span class="n">select_optimizer</span>
<span class="kn">from</span> <span class="nn">..util.general</span> <span class="kn">import</span> <span class="n">multigrid</span><span class="p">,</span> <span class="n">samples_multidimensional_uniform</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">..core.task.space</span> <span class="kn">import</span> <span class="n">Design_space</span>


<div class="viewcode-block" id="AcquisitionOptimizer"><a class="viewcode-back" href="../../../GPyOpt.optimization.html#GPyOpt.optimization.acquisition_optimizer.AcquisitionOptimizer">[docs]</a><span class="k">def</span> <span class="nf">AcquisitionOptimizer</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">current_X</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Chooser for the type of acquisition optimizer to use. The decision is based on the type of input space and</span>
<span class="sd">    whether it contains discrete, continuous, bandit variables or a mix of them. If the problem is defined for</span>
<span class="sd">    a mix of discrete and continuous variables the optimization is done as follows:</span>
<span class="sd">        - All possible combinations of the values of the discrete variables are computed.</span>
<span class="sd">        - For each combination the problem is solved for the remaining continuous variables.</span>
<span class="sd">        - The arg min of all the sub-problems is taken.</span>
<span class="sd">    Note that this may be slow in cases with many discrete variables. In the bandits settings not optimization is </span>
<span class="sd">    carried out. Since the space is finite the argmin is computed.</span>

<span class="sd">    :param space: design space class from GPyOpt.</span>
<span class="sd">    :param optimizer: optimizer to use. Can be selected among:</span>
<span class="sd">        - &#39;lbfgs&#39;: L-BFGS.</span>
<span class="sd">        - &#39;DIRECT&#39;: Dividing Rectangles.</span>
<span class="sd">        - &#39;CMA&#39;: covariance matrix adaptation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">if</span> <span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;bandit&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="p">(</span><span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;continuous&#39;</span><span class="p">]</span> <span class="ow">or</span> <span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;discrete&#39;</span><span class="p">]):</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&#39;Not possible to combine bandits with other variable types.)&#39;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;bandit&#39;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">BanditAcqOptimizer</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">current_X</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;continuous&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;discrete&#39;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">ContAcqOptimizer</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;continuous&#39;</span><span class="p">]</span> <span class="ow">and</span>  <span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;discrete&#39;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">MixedAcqOptimizer</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">elif</span> <span class="ow">not</span> <span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;continuous&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;discrete&#39;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">BanditAcqOptimizer</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">current_X</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="AcquOptimizer"><a class="viewcode-back" href="../../../GPyOpt.optimization.html#GPyOpt.optimization.acquisition_optimizer.AcquOptimizer">[docs]</a><span class="k">class</span> <span class="nc">AcquOptimizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for the optimizers of the acquisition functions.</span>

<span class="sd">    :param space: design space class from GPyOpt.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">space</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">space</span> <span class="o">=</span> <span class="n">space</span>
        
<div class="viewcode-block" id="AcquOptimizer.optimize"><a class="viewcode-back" href="../../../GPyOpt.optimization.html#GPyOpt.optimization.acquisition_optimizer.AcquOptimizer.optimize">[docs]</a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">f_df</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param f: function to optimize.</span>
<span class="sd">        :param df: gradient of the function to optimize.</span>
<span class="sd">        :param f_df: returns both the function to optimize and its gradient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
</div></div>
<div class="viewcode-block" id="ContAcqOptimizer"><a class="viewcode-back" href="../../../GPyOpt.optimization.html#GPyOpt.optimization.acquisition_optimizer.ContAcqOptimizer">[docs]</a><span class="k">class</span> <span class="nc">ContAcqOptimizer</span><span class="p">(</span><span class="n">AcquOptimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General class for acquisition optimizers defined in continuous domains </span>

<span class="sd">    :param space: design space class from GPyOpt.</span>
<span class="sd">    :param optimizer: optimizer to use. Can be selected among:</span>
<span class="sd">        - &#39;lbfgs&#39;: L-BFGS.</span>
<span class="sd">        - &#39;DIRECT&#39;: Dividing Rectangles.</span>
<span class="sd">        - &#39;CMA&#39;: covariance matrix adaptation.</span>
<span class="sd">    :param n_samples: number of initial points in which the acquisition is evaluated.</span>
<span class="sd">    :param fast: whether just a local optimizer should be run starting in the best location (default, True). If False a local search is performed</span>
<span class="sd">            for each point and the best of all is taken.</span>
<span class="sd">    :param ramdom: whether the initial samples are taken randomly (or in a grid if False). Note that, if False, n_samples represent the number</span>
<span class="sd">            of points user per dimension.</span>
<span class="sd">    :param search: whether to do local search or not.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">space</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">search</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ContAcqOptimizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">space</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_samples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fast</span><span class="o">=</span> <span class="n">fast</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random</span> <span class="o">=</span> <span class="n">random</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">search</span> <span class="o">=</span> <span class="n">search</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">select_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_name</span><span class="p">)(</span><span class="n">space</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">free_dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">space</span><span class="o">.</span><span class="n">dimensionality</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">get_bounds</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subspace</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">random</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">samples_multidimensional_uniform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">multigrid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">)</span>


<div class="viewcode-block" id="ContAcqOptimizer.fix_dimensions"><a class="viewcode-back" href="../../../GPyOpt.optimization.html#GPyOpt.optimization.acquisition_optimizer.ContAcqOptimizer.fix_dimensions">[docs]</a>    <span class="k">def</span> <span class="nf">fix_dimensions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Fix the values of some of the dimensions. Once this this done the optimization is carried out only across the not fixed dimensions.</span>

<span class="sd">        :param dims: list of the indexes of the dimensions to fix.</span>
<span class="sd">        :param values: list of the values at which the selected dimensions should be fixed.</span>
<span class="sd">        &#39;&#39;&#39;</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">fixed_dims</span> <span class="o">=</span> <span class="n">dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fixed_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
        
        <span class="c"># -- restore to initial values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">free_dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">dimensionality</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">get_bounds</span><span class="p">()</span>

        <span class="c"># -- change free dimensions and remove bounds from fixed dimensions</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fixed_dims</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span> <span class="c"># need to reverse the order to start removing from the back, otherwise dimensions dont&#39; match</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">free_dims</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="c"># -- take only the fixed components of the random samples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">samples</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">free_dims</span><span class="p">)]</span> <span class="c"># take only the component of active dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subspace</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">get_subspace</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">free_dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">select_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_name</span><span class="p">)(</span><span class="n">Design_space</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subspace</span><span class="p">),</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_expand_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Takes a value x in the subspace of not fixed dimensions and expands it with the values of the fixed ones.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">dimensionality</span><span class="p">))</span> 
        <span class="n">xx</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">free_dims</span><span class="p">)]</span>  <span class="o">=</span> <span class="n">x</span>  
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">dimensionality</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">free_dims</span><span class="p">):</span>
            <span class="n">xx</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fixed_dims</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fixed_values</span>
        <span class="k">return</span> <span class="n">xx</span>

<div class="viewcode-block" id="ContAcqOptimizer.optimize"><a class="viewcode-back" href="../../../GPyOpt.optimization.html#GPyOpt.optimization.acquisition_optimizer.ContAcqOptimizer.optimize">[docs]</a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">f_df</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Optimizes the input function.</span>

<span class="sd">        :param f: function to optimize.</span>
<span class="sd">        :param df: gradient of the function to optimize.</span>
<span class="sd">        :param f_df: returns both the function to optimize and its gradient.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f</span> <span class="o">=</span> <span class="n">f</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f_df</span> <span class="o">=</span> <span class="n">f_df</span>

        <span class="k">def</span> <span class="nf">fp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            Wrapper of *f*: takes an input x with size of the not fixed dimensions expands it and evaluates the entire function.</span>
<span class="sd">            &#39;&#39;&#39;</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">xx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_vector</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>        
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">xx</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">fp_dfp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            Wrapper of the derivative of *f*: takes an input x with size of the not fixed dimensions expands it and evaluates the gradient of the entire function.</span>
<span class="sd">            &#39;&#39;&#39;</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">xx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_vector</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>        
            
            <span class="n">fp_xx</span> <span class="p">,</span> <span class="n">dfp_xx</span> <span class="o">=</span> <span class="n">f_df</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
            <span class="n">dfp_xx</span> <span class="o">=</span> <span class="n">dfp_xx</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">free_dims</span><span class="p">)]</span>
            <span class="k">return</span> <span class="n">fp_xx</span><span class="p">,</span> <span class="n">dfp_xx</span>

        <span class="c">## --- The optimization is done here</span>

        <span class="c">## --- Fast method: only runs a local optimizer at the best found evaluation</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fast</span><span class="p">:</span>
            <span class="n">pred_fp</span> <span class="o">=</span> <span class="n">fp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">samples</span><span class="p">)</span>
            <span class="n">x0</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">pred_fp</span><span class="p">)]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">search</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">f_df</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span> <span class="n">fp_dfp</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c"># -- In case no gradients are available </span>
                <span class="n">x_min</span><span class="p">,</span> <span class="n">f_min</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span><span class="n">fp</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">f_df</span><span class="o">=</span><span class="n">fp_dfp</span><span class="p">)</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_vector</span><span class="p">(</span><span class="n">x_min</span><span class="p">),</span> <span class="n">f_min</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_vector</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x0</span><span class="p">)),</span> <span class="n">pred_fp</span>
        <span class="k">else</span><span class="p">:</span>
        <span class="c">## --- Standard method: runs a local optimizer at all the acquisition evaluation</span>
            <span class="n">x_min</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="n">f_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">Inf</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">search</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">f_df</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span> <span class="n">fp_dfp</span> <span class="o">=</span> <span class="bp">None</span> <span class="c"># -- In case no gradients are available </span>
                    <span class="n">x1</span><span class="p">,</span> <span class="n">f1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">f</span> <span class="o">=</span><span class="n">fp</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">f_df</span><span class="o">=</span><span class="n">fp_dfp</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">x1</span><span class="p">,</span> <span class="n">f1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">f1</span><span class="o">&lt;</span><span class="n">f_min</span><span class="p">:</span>
                    <span class="n">x_min</span> <span class="o">=</span> <span class="n">x1</span>
                    <span class="n">f_min</span> <span class="o">=</span> <span class="n">f1</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_vector</span><span class="p">(</span><span class="n">x_min</span><span class="p">),</span> <span class="n">f_min</span>
        
</div></div>
<div class="viewcode-block" id="BanditAcqOptimizer"><a class="viewcode-back" href="../../../GPyOpt.optimization.html#GPyOpt.optimization.acquisition_optimizer.BanditAcqOptimizer">[docs]</a><span class="k">class</span> <span class="nc">BanditAcqOptimizer</span><span class="p">(</span><span class="n">AcquOptimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General class for acquisition optimizers defined on bandits</span>

<span class="sd">    :param space: design space class from GPyOpt.</span>
<span class="sd">    :param current_X: numpy array containing the arms of the bandit that hasn&#39;t been pulled yet.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">space</span><span class="p">,</span> <span class="n">current_X</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BanditAcqOptimizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">space</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">space</span> <span class="o">=</span> <span class="n">space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pulled_arms</span> <span class="o">=</span> <span class="n">current_X</span>

<div class="viewcode-block" id="BanditAcqOptimizer.optimize"><a class="viewcode-back" href="../../../GPyOpt.optimization.html#GPyOpt.optimization.acquisition_optimizer.BanditAcqOptimizer.optimize">[docs]</a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">f_df</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Optimization of the acquisition. Since it is a bandit it just takes the argmin of the outputs.</span>

<span class="sd">        :param f: function to optimize.</span>
<span class="sd">        :param df: gradient of the function to optimize.</span>
<span class="sd">        :param f_df: returns both the function to optimize and its gradient.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c"># --- Get all potential arms</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">has_types</span><span class="p">[</span><span class="s">&#39;discrete&#39;</span><span class="p">]:</span>
            <span class="n">arms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">get_discrete_grid</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">arms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">get_bandit</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">arms</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">pulled_arms</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="c"># --- remove select best arm not yet sampled</span>
            <span class="n">pref_f</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">arms</span><span class="p">)</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">pref_f</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
            <span class="n">k</span><span class="o">=</span><span class="mi">0</span>
            <span class="k">while</span> <span class="nb">any</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">pulled_arms</span><span class="p">[:]</span><span class="o">==</span><span class="n">arms</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">k</span><span class="p">],:]</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="mi">1</span><span class="p">)):</span>
                <span class="n">k</span> <span class="o">+=</span><span class="mi">1</span> 
                
            <span class="n">x_min</span> <span class="o">=</span> <span class="n">arms</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">k</span><span class="p">],:]</span>
            <span class="n">f_min</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_min</span><span class="p">)</span>
            
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&#39;All locations of the design space have been sampled.&#39;</span><span class="p">)</span>
            <span class="c">#break</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">pulled_arms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">pulled_arms</span><span class="p">,</span> <span class="n">x_min</span><span class="p">))</span>


        <span class="c"># --- Previous approach: do not remove those already sampled</span>
        <span class="c"># pref_f = f(arms)</span>
        <span class="c"># x_min = arms[np.argmin(pref_f)]</span>
        <span class="c"># f_min = f(x_min)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x_min</span><span class="p">),</span> <span class="n">f_min</span>

</div></div>
<div class="viewcode-block" id="MixedAcqOptimizer"><a class="viewcode-back" href="../../../GPyOpt.optimization.html#GPyOpt.optimization.acquisition_optimizer.MixedAcqOptimizer">[docs]</a><span class="k">class</span> <span class="nc">MixedAcqOptimizer</span><span class="p">(</span><span class="n">AcquOptimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General class for acquisition optimizers defined on mixed domains of continuous and discrete variables. </span>

<span class="sd">    :param space: design space class from GPyOpt.</span>
<span class="sd">    :param optimizer: optimizer to use. Can be selected among:</span>
<span class="sd">        - &#39;lbfgs&#39;: L-BFGS.</span>
<span class="sd">        - &#39;DIRECT&#39;: Dividing Rectangles.</span>
<span class="sd">        - &#39;CMA&#39;: covariance matrix adaptation.</span>
<span class="sd">    :param n_samples: number of initial points in which the acquisition is evaluated.</span>
<span class="sd">    :param fast: whether just a local optimizer should be run starting in the best location (default, True). If False a local search is performed</span>
<span class="sd">            for each point and the best of all is taken.</span>
<span class="sd">    :param ramdom: whether the initial samples are taken randomly (or in a grid if False). Note that, if False, n_samples represent the number</span>
<span class="sd">            of points user per dimension.</span>
<span class="sd">    :param search: whether to do local search or not.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">space</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">search</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MixedAcqOptimizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">space</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">space</span> <span class="o">=</span> <span class="n">space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mixed_optimizer</span> <span class="o">=</span> <span class="n">ContAcqOptimizer</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="n">fast</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="n">random</span><span class="p">,</span> <span class="n">search</span><span class="o">=</span><span class="n">search</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discrete_dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">get_discrete_dims</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discrete_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">get_discrete_grid</span><span class="p">()</span>

<div class="viewcode-block" id="MixedAcqOptimizer.optimize"><a class="viewcode-back" href="../../../GPyOpt.optimization.html#GPyOpt.optimization.acquisition_optimizer.MixedAcqOptimizer.optimize">[docs]</a>    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">f_df</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Optimization of the acquisition. </span>

<span class="sd">        :param f: function to optimize.</span>
<span class="sd">        :param df: gradient of the function to optimize.</span>
<span class="sd">        :param f_df: returns both the function to optimize and its gradient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_discrete</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete_values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">partial_x_min</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_discrete</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">dimensionality</span><span class="p">))</span>
        <span class="n">partial_f_min</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_discrete</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_discrete</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mixed_optimizer</span><span class="o">.</span><span class="n">fix_dimensions</span><span class="p">(</span><span class="n">dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">discrete_dims</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">discrete_values</span><span class="p">[</span><span class="n">i</span><span class="p">,:])</span>
            <span class="n">partial_x_min</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="p">,</span> <span class="n">partial_f_min</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">f_df</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">partial_x_min</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">partial_f_min</span><span class="p">)]),</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">partial_f_min</span><span class="p">))</span>
</pre></div></div></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">GPyOpt  documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2015, Javier Gonzalez.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>