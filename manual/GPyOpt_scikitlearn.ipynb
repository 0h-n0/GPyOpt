{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPyOpt: configuring Scikit-learn methods\n",
    "\n",
    "### Written by Javier Gonzalez and Zhenwen Dai, University of Sheffield.\n",
    "\n",
    "*Last updated Tuesday, 15 March 2016.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to use GPyOpt to tune the parameters of Machine Learning algorithms. In particular, we will shows how to tune the hyper-parameters for the [Support Vector Regression (SVR)](http://www.svms.org/regression/SmSc98.pdf) implemented in [Scikit-learn](http://scikit-learn.org/stable/). Given the standard interface of Scikit-learn, other models can be tuned in a similar fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start loading the requires modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline  \n",
    "import GPy\n",
    "import GPyOpt\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from numpy.random import seed\n",
    "seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this example we will use the Olympic marathon dataset available in GPy. \n",
    "We split the original dataset into the training data (first 20 data points) and testing data (last 7 data points). The performance of SVR is evaluated in terms of Rooted Mean Squared Error (RMSE) on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's load the dataset\n",
    "GPy.util.datasets.authorize_download = lambda x: True # prevents requesting authorization for download.\n",
    "data = GPy.util.datasets.olympic_marathon_men()\n",
    "X = data['X']\n",
    "Y = data['Y']\n",
    "X_train = X[:20]\n",
    "Y_train = Y[:20,0]\n",
    "X_test = X[20:]\n",
    "Y_test = Y[20:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see the results with the default kernel parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svr = svm.SVR()\n",
    "svr.fit(X_train,Y_train)\n",
    "Y_train_pred = svr.predict(X_train)\n",
    "Y_test_pred = svr.predict(X_test)\n",
    "print(\"The default parameters obtained: C=\"+str(svr.C)+\", epilson=\"+str(svr.epsilon)+\", gamma=\"+str(svr.gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the RMSE on the testing data and plot the prediction. With the default parameters, SVR does not give an OK fit to the training data but completely miss out the testing data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(X_train,Y_train_pred,'b',label='pred-train')\n",
    "plot(X_test,Y_test_pred,'g',label='pred-test')\n",
    "plot(X_train,Y_train,'rx',label='ground truth')\n",
    "plot(X_test,Y_test,'rx')\n",
    "legend(loc='best')\n",
    "print(\"RMSE = \"+str(np.sqrt(np.square(Y_test_pred-Y_test).mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try Bayesian Optimization. We first write a wrap function for fitting with SVR. The objective is the RMSE from cross-validation. We optimize the parameters in *log* space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfold = 3\n",
    "def fit_svr_val(x):\n",
    "    x = np.atleast_2d(np.exp(x))\n",
    "    fs = np.zeros((x.shape[0],1))\n",
    "    for i in range(x.shape[0]):\n",
    "        fs[i] = 0\n",
    "        for n in range(nfold):\n",
    "            idx = np.array(range(X_train.shape[0]))\n",
    "            idx_valid = np.logical_and(idx>=X_train.shape[0]/nfold*n, idx<X_train.shape[0]/nfold*(n+1))\n",
    "            idx_train = np.logical_not(idx_valid)\n",
    "            svr = svm.SVR(C=x[i,0], epsilon=x[i,1],gamma=x[i,2])\n",
    "            svr.fit(X_train[idx_train],Y_train[idx_train])\n",
    "            fs[i] += np.sqrt(np.square(svr.predict(X_train[idx_valid])-Y_train[idx_valid]).mean())\n",
    "        fs[i] *= 1./nfold\n",
    "    return fs\n",
    "\n",
    "## -- Note that similar wrapper functions can be used to tune other Scikit-learn methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the search interval of $C$ to be roughly $[0,1000]$ and the search interval of $\\epsilon$ and $\\gamma$ to be roughtly $[1\\times 10^{-5},0.1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain       =[{'name': 'C',      'type': 'continuous', 'domain': (0.,7.)},\n",
    "               {'name': 'epsilon','type': 'continuous', 'domain': (-12.,-2.)},\n",
    "               {'name': 'gamma',  'type': 'continuous', 'domain': (-12.,-2.)}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, then, create the GPyOpt object and run the optimization procedure. It might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = GPyOpt.methods.BayesianOptimization(f = fit_svr_val,            # function to optimize       \n",
    "                                          domain = domain,         # box-constrains of the problem\n",
    "                                          acquisition_type ='LCB',       # LCB acquisition\n",
    "                                          acquisition_weight = 0.1)   # Exploration exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# it may take a few seconds\n",
    "opt.run_optimization(max_iter=50)\n",
    "opt.plot_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the best parameters found. They differ significantly from the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_best = np.exp(opt.X[np.argmin(opt.Y)])\n",
    "print(\"The best parameters obtained: C=\"+str(x_best[0])+\", epilson=\"+str(x_best[1])+\", gamma=\"+str(x_best[2]))\n",
    "svr = svm.SVR(C=x_best[0], epsilon=x_best[1],gamma=x_best[2])\n",
    "svr.fit(X_train,Y_train)\n",
    "Y_train_pred = svr.predict(X_train)\n",
    "Y_test_pred = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see SVR does a reasonable fit to the data. The result could be further improved by increasing the *max_iter*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(X_train,Y_train_pred,'b',label='pred-train')\n",
    "plot(X_test,Y_test_pred,'g',label='pred-test')\n",
    "plot(X_train,Y_train,'rx',label='ground truth')\n",
    "plot(X_test,Y_test,'rx')\n",
    "legend(loc='best')\n",
    "print(\"RMSE = \"+str(np.sqrt(np.square(Y_test_pred-Y_test).mean())))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
